# Jupyter Notebooks para An√°lisis de Datos

Jupyter Notebooks son excelentes para explorar datos, hacer an√°lisis y documentar tu trabajo.

---

## üß† ¬øQu√© es Jupyter?

Jupyter Notebook es un entorno interactivo que combina:
* **C√≥digo ejecutable** (Python, SQL, etc.)
* **Resultados visuales** (gr√°ficos, tablas)
* **Documentaci√≥n** (Markdown, texto)
* **Todo en un solo lugar**

> Jupyter es perfecto para explorar datos, hacer an√°lisis y compartir resultados.

---

## üöÄ Empezar con Jupyter Notebooks

> üí° **Si ya instalaste Jupyter en Fundamentos**: Puedes saltar esta secci√≥n y ir directo a "Flujo de trabajo t√≠pico".

### Si est√°s usando Cursor

1. **Abre el notebook de ejemplo**: `03_python/ejemplos/00-notebook.ipynb`
2. **Selecciona el kernel de Python** en la parte superior
3. **¬°Empieza a trabajar!**

### Si NO est√°s usando Cursor: Instalaci√≥n

```bash
# Con pip
pip install jupyter pandas matplotlib seaborn

# O con conda
conda install jupyter pandas matplotlib seaborn
```

### Iniciar Jupyter

```bash
# Iniciar servidor
jupyter notebook

# O JupyterLab (interfaz moderna)
jupyter lab
```

Se abrir√° en tu navegador en `http://localhost:8888`

---

## üìì Estructura de un Notebook

Un notebook tiene **celdas** que pueden ser:

### Celda de c√≥digo

```python
import pandas as pd
import matplotlib.pyplot as plt

# Cargar datos desde el CSV de ejemplo
df = pd.read_csv('../data/ventas.csv')
df.head()
```

### Celda de Markdown

```markdown
# An√°lisis de Ventas

Este notebook analiza las ventas del √∫ltimo trimestre.

## Objetivos
1. Calcular totales por categor√≠a
2. Identificar tendencias
3. Generar visualizaciones
```

---

## üîç Flujo de trabajo t√≠pico

### 1. Cargar y explorar datos

```python
# Celda 1: Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Celda 2: Cargar datos desde el CSV de ejemplo
# El archivo est√° en 03_python/data/ventas.csv
df = pd.read_csv('../data/ventas.csv')
df.head()

# Celda 3: Explorar
df.info()
df.describe()

# Celda 4: Verificar nulos
df.isnull().sum()
```

### 2. Limpiar datos

```python
# Celda 5: Limpiar
df = df.dropna()
df = df.drop_duplicates()
df['fecha'] = pd.to_datetime(df['fecha'])

# Verificar que la limpieza funcion√≥
print(f"Registros despu√©s de limpieza: {len(df)}")
df.head()
```

### 3. An√°lisis

```python
# Celda 6: Agregaciones
ventas_por_categoria = df.groupby('categoria')['total'].sum()
print("Ventas por categor√≠a:")
print(ventas_por_categoria)

# Celda 7: Visualizaci√≥n
ventas_por_categoria.plot(kind='bar', figsize=(10, 6))
plt.title('Ventas por Categor√≠a')
plt.xlabel('Categor√≠a')
plt.ylabel('Total de Ventas (‚Ç¨)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

### 4. Conclusiones

```markdown
## Conclusiones

- La categor√≠a "Electr√≥nica" tiene las mayores ventas
- Las ventas est√°n distribuidas entre Madrid, Barcelona, Valencia y Sevilla
- Recomendaci√≥n: Analizar tendencias por ciudad y mes
```

---

## üîç Exploraci√≥n de Datos (EDA)

EDA (Exploratory Data Analysis) es el proceso de entender la estructura de tus datos, identificar patrones y detectar problemas de calidad.

> üí° **Ejemplo pr√°ctico**: Usa el CSV de ejemplo `../data/ventas.csv` para practicar estos conceptos.

### Primeros pasos

```python
# Cargar datos
df = pd.read_csv('../data/ventas.csv')

# Primer vistazo
df.head()        # Primeras 5 filas
df.tail()        # √öltimas 5 filas
df.sample(5)     # 5 filas aleatorias

# Informaci√≥n b√°sica
print(f"Filas: {df.shape[0]}, Columnas: {df.shape[1]}")
df.info()        # Tipos, memoria, nulos
df.describe()    # Solo columnas num√©ricas
```

### Exploraci√≥n de estructura

```python
# Ver todas las columnas
print("Columnas:", df.columns.tolist())

# Tipos de datos
print("\nTipos de datos:")
print(df.dtypes)

# Valores √∫nicos por columna
for col in df.columns:
    print(f"\n{col}:")
    print(f"  Valores √∫nicos: {df[col].nunique()}")
    if df[col].nunique() < 20:  # Si hay pocos valores √∫nicos
        print(f"  Valores: {df[col].unique()}")
```

### Estad√≠sticas descriptivas

```python
# Estad√≠sticas b√°sicas
df.describe()

# Estad√≠sticas por columna num√©rica
df['precio'].describe()

# Estad√≠sticas personalizadas
df.agg({
    'precio': ['mean', 'median', 'std', 'min', 'max'],
    'cantidad': ['sum', 'mean']
})

# Percentiles
df['precio'].quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99])
```

### Detectar problemas

#### Valores nulos

```python
# Contar nulos por columna
print("Valores nulos:")
print(df.isnull().sum())

# Porcentaje de nulos
print("\nPorcentaje de nulos:")
print((df.isnull().sum() / len(df) * 100).sort_values(ascending=False))

# Filas con nulos
df[df.isnull().any(axis=1)]

# Visualizar nulos (si hay muchos)
import seaborn as sns
sns.heatmap(df.isnull(), cbar=True, yticklabels=False)
plt.title('Mapa de Valores Nulos')
plt.show()
```

#### Duplicados

```python
# Detectar duplicados completos
duplicados = df[df.duplicated()]
print(f"Duplicados completos: {len(duplicados)}")

# Duplicados en columnas espec√≠ficas
duplicados_id = df[df.duplicated(subset=['id'])]
print(f"IDs duplicados: {len(duplicados_id)}")
```

#### Valores at√≠picos (Outliers)

```python
# Funci√≥n para detectar outliers usando IQR
def detectar_outliers(df, columna):
    Q1 = df[columna].quantile(0.25)
    Q3 = df[columna].quantile(0.75)
    IQR = Q3 - Q1
    
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR
    
    outliers = df[(df[columna] < limite_inferior) | (df[columna] > limite_superior)]
    return outliers

# Detectar outliers en precio
outliers_precio = detectar_outliers(df, 'precio')
print(f"Outliers en precio: {len(outliers_precio)}")
if len(outliers_precio) > 0:
    print(outliers_precio[['id', 'producto', 'precio']])
```

### An√°lisis de relaciones

#### Correlaciones

```python
# Matriz de correlaci√≥n (solo columnas num√©ricas)
columnas_numericas = df.select_dtypes(include=[np.number]).columns
correlaciones = df[columnas_numericas].corr()
print(correlaciones)

# Visualizar correlaciones
plt.figure(figsize=(8, 6))
sns.heatmap(correlaciones, annot=True, cmap='coolwarm', center=0)
plt.title('Matriz de Correlaci√≥n')
plt.tight_layout()
plt.show()
```

#### Agrupaciones

```python
# Agrupar y analizar
df.groupby('categoria')['precio'].agg(['mean', 'median', 'std', 'count'])

# M√∫ltiples agrupaciones
df['mes'] = pd.to_datetime(df['fecha']).dt.to_period('M')
df.groupby(['categoria', 'mes'])['total'].sum()
```

### Visualizaci√≥n para exploraci√≥n

#### Histogramas

```python
# Histograma de precios
df['precio'].hist(bins=20, figsize=(10, 6))
plt.xlabel('Precio (‚Ç¨)')
plt.ylabel('Frecuencia')
plt.title('Distribuci√≥n de Precios')
plt.tight_layout()
plt.show()
```

#### Box plots

```python
# Box plot para detectar outliers por categor√≠a
df.boxplot(column='precio', by='categoria', figsize=(10, 6))
plt.title('Distribuci√≥n de Precios por Categor√≠a')
plt.suptitle('')  # Eliminar t√≠tulo autom√°tico
plt.xlabel('Categor√≠a')
plt.ylabel('Precio (‚Ç¨)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

#### Scatter plots

```python
# Relaci√≥n entre cantidad y precio
plt.figure(figsize=(10, 6))
plt.scatter(df['cantidad'], df['precio'], alpha=0.6)
plt.xlabel('Cantidad')
plt.ylabel('Precio (‚Ç¨)')
plt.title('Relaci√≥n Cantidad vs Precio')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

### Flujo de exploraci√≥n completo

```python
# 1. Carga y primera inspecci√≥n
df = pd.read_csv('../data/ventas.csv')
print(f"Shape: {df.shape}")
print(f"\nColumnas: {df.columns.tolist()}")
print(f"\nTipos:\n{df.dtypes}")
print(f"\nPrimeras filas:\n{df.head()}")

# 2. Estad√≠sticas b√°sicas
print("\n=== Estad√≠sticas Num√©ricas ===")
print(df.describe())

print("\n=== Estad√≠sticas Categ√≥ricas ===")
for col in df.select_dtypes(include=['object']).columns:
    print(f"\n{col}:")
    print(df[col].value_counts().head(10))

# 3. Detectar problemas
print("\n=== Problemas Detectados ===")
print("Valores nulos:")
print(df.isnull().sum())
print(f"\nDuplicados: {df.duplicated().sum()}")

# 4. An√°lisis de relaciones
print("\n=== Correlaciones ===")
columnas_numericas = df.select_dtypes(include=[np.number]).columns
correlaciones = df[columnas_numericas].corr()
print(correlaciones)

# 5. Agrupaciones interesantes
print("\n=== Ventas por Categor√≠a ===")
print(df.groupby('categoria')['total'].agg(['sum', 'mean', 'count']))
```

### Tips para EDA efectivo

1. **Empieza con lo b√°sico**: Siempre ejecuta `df.head()`, `df.info()`, `df.describe()`
2. **Documenta tus hallazgos**: En Jupyter Notebook, usa celdas Markdown para documentar
3. **Visualiza siempre**: Los n√∫meros cuentan, las visualizaciones muestran
4. **Formula preguntas**: ¬øQu√© patrones veo? ¬øHay anomal√≠as? ¬øQu√© relaciones existen?

---

## üìä Visualizaciones

### Matplotlib b√°sico

```python
import matplotlib.pyplot as plt

# Gr√°fico de barras
df.groupby('categoria')['total'].sum().plot(kind='bar', figsize=(10, 6))
plt.title('Ventas por Categor√≠a')
plt.xlabel('Categor√≠a')
plt.ylabel('Total (‚Ç¨)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Gr√°fico de l√≠neas (ventas por mes)
df['mes'] = pd.to_datetime(df['fecha']).dt.to_period('M')
ventas_mes = df.groupby('mes')['total'].sum()
ventas_mes.plot(kind='line', figsize=(10, 6), marker='o')
plt.title('Ventas por Mes')
plt.xlabel('Mes')
plt.ylabel('Total (‚Ç¨)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Histograma de precios
df['precio'].hist(bins=20, figsize=(10, 6))
plt.title('Distribuci√≥n de Precios')
plt.xlabel('Precio (‚Ç¨)')
plt.ylabel('Frecuencia')
plt.tight_layout()
plt.show()
```

### Seaborn (m√°s bonito)

```python
import seaborn as sns

# Configurar estilo
sns.set_style("whitegrid")

# Gr√°fico de barras
plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='categoria', y='total')
plt.title('Ventas por Categor√≠a')
plt.xlabel('Categor√≠a')
plt.ylabel('Total (‚Ç¨)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Box plot de precios por categor√≠a
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='categoria', y='precio')
plt.title('Distribuci√≥n de Precios por Categor√≠a')
plt.xlabel('Categor√≠a')
plt.ylabel('Precio (‚Ç¨)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Heatmap de correlaci√≥n (solo columnas num√©ricas)
columnas_numericas = df.select_dtypes(include=['float64', 'int64']).columns
plt.figure(figsize=(8, 6))
sns.heatmap(df[columnas_numericas].corr(), annot=True, cmap='coolwarm', center=0)
plt.title('Correlaci√≥n entre Variables Num√©ricas')
plt.tight_layout()
plt.show()
```

---

## üîó Integraci√≥n con SQL

### Conectar a base de datos

> ‚ö†Ô∏è **Importante**: Antes de ejecutar este c√≥digo, aseg√∫rate de que **Docker est√© corriendo** con la base de datos PostgreSQL. Si no lo has iniciado, ve a `02_sql/` y ejecuta `docker-compose up -d`.

```python
from sqlalchemy import create_engine
import pandas as pd
import os
from dotenv import load_dotenv
from pathlib import Path

# Cargar variables de entorno desde .env en la ra√≠z del proyecto
env_path = Path().resolve().parent.parent / '.env'
load_dotenv(env_path)

def conectar_db():
    """Crea conexi√≥n a la base de datos usando variables del .env."""
    # Opci√≥n 1: Usar DATABASE_URL si est√° disponible (m√°s simple)
    database_url = os.getenv('DATABASE_URL')
    if database_url:
        engine = create_engine(database_url)
        return engine
    
    # Opci√≥n 2: Construir desde variables individuales (fallback)
    db_host = os.getenv('DB_HOST', 'localhost')
    db_port = os.getenv('DB_PORT', '5432')
    db_name = os.getenv('DB_NAME', 'data_engineering')
    db_user = os.getenv('DB_USER', 'de_user')
    db_password = os.getenv('DB_PASSWORD', 'de_password')
    
    connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'
    engine = create_engine(connection_string)
    return engine

# Conectar a la base de datos
engine = conectar_db()

# Ejecutar query usando las tablas de ejemplo (usuarios, productos, ventas)
query = """
SELECT 
    p.categoria,
    COUNT(v.id) AS total_ventas,
    SUM(v.total) AS ingresos_totales
FROM productos p
LEFT JOIN ventas v ON p.id = v.producto_id
GROUP BY p.categoria
ORDER BY ingresos_totales DESC
"""

df = pd.read_sql(query, engine)
df
```

### Ejemplo completo: An√°lisis de ventas desde la base de datos

```python
# Cargar datos de la base de datos
query_ventas = """
SELECT 
    u.nombre,
    u.ciudad,
    p.categoria,
    p.nombre AS producto,
    v.cantidad,
    v.total,
    v.fecha_venta
FROM ventas v
JOIN usuarios u ON v.usuario_id = u.id
JOIN productos p ON v.producto_id = p.id
ORDER BY v.fecha_venta DESC
"""

df_ventas = pd.read_sql(query_ventas, engine)
print(f"Total de registros: {len(df_ventas)}")
df_ventas.head()

# An√°lisis con pandas
ventas_por_categoria = df_ventas.groupby('categoria')['total'].sum()
print("\nVentas por categor√≠a:")
print(ventas_por_categoria)

# Visualizaci√≥n
ventas_por_categoria.plot(kind='bar', figsize=(10, 6))
plt.title('Ventas por Categor√≠a (desde Base de Datos)')
plt.xlabel('Categor√≠a')
plt.ylabel('Total (‚Ç¨)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

> üí° **Tip**: Si tienes problemas de conexi√≥n, verifica que:
> 1. **Docker est√© corriendo**: `docker ps` (deber√≠as ver `ing-datos-db`)
> 2. El **`.env`** est√© en la ra√≠z del proyecto con las credenciales correctas
> 3. El **puerto** en `DB_PORT` o `POSTGRES_PORT` coincida con el que usa Docker (por defecto: 5432, o 15432 si cambiaste el puerto)
> 4. Si Docker no est√° corriendo, ve a `02_sql/` y ejecuta: `docker-compose up -d`

### Magic commands (Opcional)

Si prefieres ejecutar SQL directamente en el notebook sin pandas, puedes usar magic commands:

> ‚ö†Ô∏è **Importante**: Necesitas instalar la extensi√≥n `ipython-sql` primero: `pip install ipython-sql`

```python
# Cargar variables de entorno
import os
from dotenv import load_dotenv
from pathlib import Path

env_path = Path().resolve().parent.parent / '.env'
load_dotenv(env_path)

# Construir connection string desde .env
db_host = os.getenv('DB_HOST', 'localhost')
db_port = os.getenv('DB_PORT', '5432')
db_name = os.getenv('DB_NAME', 'data_engineering')
db_user = os.getenv('DB_USER', 'de_user')
db_password = os.getenv('DB_PASSWORD', 'de_password')

# Cargar extensi√≥n SQL
%load_ext sql

# Conectar usando variables del .env
connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'
%sql $connection_string

# Ejecutar queries SQL directamente
%%sql
SELECT 
    p.categoria,
    COUNT(v.id) AS total_ventas,
    SUM(v.total) AS ingresos_totales
FROM productos p
LEFT JOIN ventas v ON p.id = v.producto_id
GROUP BY p.categoria
ORDER BY ingresos_totales DESC
LIMIT 10;
```

> üí° **Tip**: Los magic commands son √∫tiles para ejecutar SQL r√°pido, pero para an√°lisis m√°s complejos es mejor usar `pandas.read_sql()` como en el ejemplo anterior.

---

## üí° Buenas pr√°cticas

### 1. Organiza tu notebook

```markdown
# 1. Introducci√≥n
# 2. Cargar datos
# 3. Exploraci√≥n
# 4. Limpieza
# 5. An√°lisis
# 6. Visualizaciones
# 7. Conclusiones
```

### 2. Documenta tu proceso

```markdown
## Paso 1: Cargar datos

Cargamos los datos de ventas del √∫ltimo trimestre.
Nota: Los datos vienen de la API de ventas.
```

### 3. Limpia outputs antes de commitear

```bash
# Limpiar outputs
jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace notebook.ipynb
```

### 4. Convierte a otros formatos

```bash
# A HTML
jupyter nbconvert notebook.ipynb --to html

# A PDF
jupyter nbconvert notebook.ipynb --to pdf

# A Python script
jupyter nbconvert notebook.ipynb --to python
```

---

## üéØ Casos de uso

### Exploraci√≥n de datos

```python
# Cargar datos primero
df = pd.read_csv('../data/ventas.csv')

# Explora r√°pidamente
df.head()
df.describe()
df['categoria'].value_counts()
df['ciudad'].value_counts()

# Gr√°fico r√°pido
df.groupby('categoria')['total'].sum().plot(kind='bar')
plt.show()
```

### An√°lisis ad-hoc

```python
# Primero carga los datos
df = pd.read_csv('../data/ventas.csv')

# Prueba diferentes enfoques
# Celda 1: Enfoque A - Ventas por categor√≠a
resultado_a = df.groupby('categoria')['total'].sum()
print("Ventas por categor√≠a:")
print(resultado_a)

# Celda 2: Enfoque B - Ventas por categor√≠a y ciudad (modifica y ejecuta)
resultado_b = df.groupby(['categoria', 'ciudad'])['total'].sum()
print("\nVentas por categor√≠a y ciudad:")
print(resultado_b)
```

### Documentaci√≥n de an√°lisis

Combina c√≥digo, resultados y explicaciones en un solo documento.

---

## ‚ö†Ô∏è Cu√°ndo NO usar Notebooks

### ‚ùå No uses notebooks para:

* **C√≥digo de producci√≥n**: Usa scripts Python
* **Pipelines automatizados**: Usa scripts o Airflow
* **C√≥digo reutilizable**: Usa m√≥dulos Python
* **Testing**: Usa frameworks de testing

### ‚úÖ Usa notebooks para:

* Exploraci√≥n de datos
* An√°lisis ad-hoc
* Prototipado r√°pido
* Documentaci√≥n de an√°lisis
* Presentaciones interactivas

---

## üîß Extensiones √∫tiles

### Instalar extensiones

```bash
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
```

### Extensiones recomendadas:

* **Table of Contents**: √çndice autom√°tico
* **Variable Inspector**: Ver variables activas
* **Code Folding**: Plegar c√≥digo
* **ExecuteTime**: Tiempo de ejecuci√≥n

---

## üéì Pr√≥ximos pasos

1. **Abre el notebook de ejemplo**: `03_python/ejemplos/00-notebook.ipynb`
2. **Carga el CSV de ejemplo**: `df = pd.read_csv('../data/ventas.csv')`
3. **Practica los ejemplos**: Ejecuta las celdas de este documento paso a paso
4. **Experimenta**: Modifica los ejemplos y crea tus propios an√°lisis
5. **Contin√∫a con Pandas**: Aprende los fundamentos de Pandas en [Introducci√≥n a Pandas](python-para-datos/01-introduccion-pandas.md)

---

## üí° Tips

* **Guarda frecuentemente**: Ctrl+S
* **Reinicia kernel** si algo se comporta raro
* **Limpia outputs** antes de compartir
* **Usa Markdown** para documentar
* **Organiza** con headers y secciones

---

> **Recuerda**: Notebooks son para explorar y analizar. Para producci√≥n, usa scripts Python organizados en m√≥dulos.
