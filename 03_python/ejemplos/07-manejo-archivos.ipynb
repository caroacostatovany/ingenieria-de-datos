{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manejo de Archivos en Python\n",
        "\n",
        "Este notebook demuestra cÃ³mo leer y escribir diferentes formatos de archivos: CSV, Parquet, JSON, Excel y mÃ¡s.\n",
        "\n",
        "**Referencia:** [Manejo de Archivos](../fundamentos/manejo-de-archivos.md)\n",
        "\n",
        "**Objetivos:**\n",
        "- Leer y escribir CSV\n",
        "- Trabajar con Parquet (formato optimizado para analytics)\n",
        "- Convertir entre formatos\n",
        "- Comparar rendimiento y tamaÃ±os\n",
        "- Procesar archivos grandes con chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importar librerÃ­as"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"âœ… LibrerÃ­as importadas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Leer CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer CSV bÃ¡sico\n",
        "df_csv = pd.read_csv('../data/ventas.csv')\n",
        "print(f\"âœ… CSV cargado: {len(df_csv)} registros\")\n",
        "print(f\"Columnas: {df_csv.columns.tolist()}\")\n",
        "df_csv.head()\n",
        "\n",
        "# Leer CSV con opciones\n",
        "print(\"\\n=== LEER CSV CON OPCIONES ===\")\n",
        "df_csv_opciones = pd.read_csv(\n",
        "    '../data/ventas.csv',\n",
        "    usecols=['producto', 'precio', 'total'],  # Solo estas columnas\n",
        "    dtype={'precio': 'float32'}  # Tipo especÃ­fico\n",
        ")\n",
        "print(f\"Columnas seleccionadas: {df_csv_opciones.columns.tolist()}\")\n",
        "print(f\"Tipo de precio: {df_csv_opciones['precio'].dtype}\")\n",
        "df_csv_opciones.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Leer y escribir Parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer Parquet\n",
        "start = time.time()\n",
        "df_parquet = pd.read_parquet('../data/ventas.parquet')\n",
        "time_parquet = time.time() - start\n",
        "\n",
        "print(f\"âœ… Parquet cargado: {len(df_parquet)} registros en {time_parquet:.4f}s\")\n",
        "df_parquet.head()\n",
        "\n",
        "# Comparar velocidad con CSV\n",
        "start = time.time()\n",
        "df_csv_test = pd.read_csv('../data/ventas.csv')\n",
        "time_csv = time.time() - start\n",
        "\n",
        "print(f\"\\nðŸ“Š COMPARACIÃ“N DE VELOCIDAD:\")\n",
        "print(f\"CSV: {time_csv:.4f}s\")\n",
        "print(f\"Parquet: {time_parquet:.4f}s\")\n",
        "if time_parquet > 0:\n",
        "    print(f\"Parquet es {time_csv/time_parquet:.2f}x mÃ¡s rÃ¡pido\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escribir Parquet\n",
        "df_csv = pd.read_csv('../data/ventas.csv')\n",
        "\n",
        "# Crear directorio de salida si no existe\n",
        "os.makedirs('../data/output', exist_ok=True)\n",
        "\n",
        "# Convertir a Parquet con opciones\n",
        "df_csv.to_parquet(\n",
        "    '../data/output/ventas_convertido.parquet',\n",
        "    engine='pyarrow',\n",
        "    compression='snappy',\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"âœ… Parquet guardado exitosamente en ../data/output/\")\n",
        "\n",
        "# Comparar tamaÃ±os\n",
        "size_csv = os.path.getsize('../data/ventas.csv')\n",
        "size_parquet = os.path.getsize('../data/ventas.parquet')\n",
        "size_parquet_conv = os.path.getsize('../data/output/ventas_convertido.parquet')\n",
        "\n",
        "print(f\"\\nðŸ“Š COMPARACIÃ“N DE TAMAÃ‘OS:\")\n",
        "print(f\"CSV: {size_csv:,} bytes ({size_csv/1024:.2f} KB)\")\n",
        "print(f\"Parquet original: {size_parquet:,} bytes ({size_parquet/1024:.2f} KB)\")\n",
        "print(f\"Parquet convertido: {size_parquet_conv:,} bytes ({size_parquet_conv/1024:.2f} KB)\")\n",
        "if size_parquet > 0:\n",
        "    print(f\"Parquet es {size_csv/size_parquet:.2f}x mÃ¡s pequeÃ±o que CSV\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Leer Parquet con opciones avanzadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer solo columnas especÃ­ficas (mÃ¡s eficiente)\n",
        "df_parquet_cols = pd.read_parquet(\n",
        "    '../data/ventas.parquet',\n",
        "    columns=['producto', 'precio', 'total']\n",
        ")\n",
        "\n",
        "print(\"=== PARQUET CON COLUMNAS ESPECÃFICAS ===\")\n",
        "print(f\"Columnas: {df_parquet_cols.columns.tolist()}\")\n",
        "print(f\"Registros: {len(df_parquet_cols)}\")\n",
        "df_parquet_cols.head()\n",
        "\n",
        "# Nota: Los filtros pushdown requieren que la columna estÃ© en el formato correcto\n",
        "# En este ejemplo, podrÃ­amos filtrar por fecha si fuera necesario\n",
        "print(\"\\nðŸ’¡ Tip: Parquet permite filtros pushdown para leer solo datos necesarios\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Trabajar con JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer CSV y convertir a JSON\n",
        "df = pd.read_csv('../data/ventas.csv')\n",
        "\n",
        "# Crear directorio de salida si no existe\n",
        "os.makedirs('../data/output', exist_ok=True)\n",
        "\n",
        "# Escribir JSON con diferentes orientaciones\n",
        "df.to_json('../data/output/ventas.json', orient='records', index=False)\n",
        "print(\"âœ… JSON guardado en ../data/output/ (orient='records')\")\n",
        "\n",
        "# Leer JSON\n",
        "df_json = pd.read_json('../data/output/ventas.json')\n",
        "print(f\"\\nâœ… JSON cargado: {len(df_json)} registros\")\n",
        "df_json.head()\n",
        "\n",
        "# Comparar tamaÃ±os\n",
        "size_json = os.path.getsize('../data/output/ventas.json')\n",
        "print(f\"\\nðŸ“Š TamaÃ±o JSON: {size_json:,} bytes ({size_json/1024:.2f} KB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Procesar archivos grandes (simulado con chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simular procesamiento de archivo grande usando chunks\n",
        "# En este ejemplo, procesamos el CSV en chunks pequeÃ±os para demostrar la tÃ©cnica\n",
        "\n",
        "chunk_size = 10  # Chunks pequeÃ±os para el ejemplo\n",
        "chunks_procesados = []\n",
        "\n",
        "print(\"=== PROCESAR CSV EN CHUNKS ===\")\n",
        "for i, chunk in enumerate(pd.read_csv('../data/ventas.csv', chunksize=chunk_size)):\n",
        "    # Procesar cada chunk: filtrar ventas con total > 500\n",
        "    chunk_filtrado = chunk[chunk['total'] > 500]\n",
        "    chunks_procesados.append(chunk_filtrado)\n",
        "    print(f\"Chunk {i+1}: {len(chunk)} registros â†’ {len(chunk_filtrado)} filtrados\")\n",
        "\n",
        "# Combinar resultados\n",
        "df_final = pd.concat(chunks_procesados, ignore_index=True)\n",
        "print(f\"\\nâœ… Total de registros despuÃ©s de procesar: {len(df_final)}\")\n",
        "print(f\"Registros con total > 500: {len(df_final)}\")\n",
        "df_final.head()\n",
        "\n",
        "# Crear directorio de salida si no existe\n",
        "os.makedirs('../data/output', exist_ok=True)\n",
        "\n",
        "# Guardar resultado en Parquet\n",
        "df_final.to_parquet('../data/output/ventas_filtradas.parquet', index=False)\n",
        "print(\"\\nâœ… Resultado guardado en ../data/output/ventas_filtradas.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Resumen: ComparaciÃ³n de formatos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"RESUMEN: COMPARACIÃ“N DE FORMATOS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Comparar tamaÃ±os\n",
        "archivos = {\n",
        "    'CSV': '../data/ventas.csv',\n",
        "    'Parquet': '../data/ventas.parquet',\n",
        "    'JSON': '../data/output/ventas.json'  # Archivo generado en este notebook\n",
        "}\n",
        "\n",
        "print(\"\\nðŸ“Š TAMAÃ‘OS DE ARCHIVOS:\")\n",
        "for formato, ruta in archivos.items():\n",
        "    if os.path.exists(ruta):\n",
        "        size = os.path.getsize(ruta)\n",
        "        print(f\"{formato:10s}: {size:>8,} bytes ({size/1024:>6.2f} KB)\")\n",
        "\n",
        "# Comparar velocidades de lectura\n",
        "print(\"\\nâš¡ VELOCIDADES DE LECTURA:\")\n",
        "for formato, ruta in archivos.items():\n",
        "    if os.path.exists(ruta):\n",
        "        start = time.time()\n",
        "        if formato == 'CSV':\n",
        "            pd.read_csv(ruta)\n",
        "        elif formato == 'Parquet':\n",
        "            pd.read_parquet(ruta)\n",
        "        elif formato == 'JSON':\n",
        "            pd.read_json(ruta)\n",
        "        tiempo = time.time() - start\n",
        "        print(f\"{formato:10s}: {tiempo:>8.4f}s\")\n",
        "\n",
        "print(\"\\nðŸ’¡ RECOMENDACIONES:\")\n",
        "print(\"âœ… CSV: Para intercambio de datos, lectura humana\")\n",
        "print(\"âœ… Parquet: Para datos procesados, analytics, pipelines\")\n",
        "print(\"âœ… JSON: Para APIs, integraciones web\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ingenieria-de-datos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
