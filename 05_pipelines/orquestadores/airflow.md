# Apache Airflow: El Est√°ndar de la Industria

Apache Airflow es el orquestador de workflows m√°s popular para Data Engineering. Aprende los conceptos b√°sicos y c√≥mo usarlo.

---

## üß† ¬øQu√© es Airflow?

Apache Airflow es una plataforma para:
* **Programar** workflows de datos
* **Orquestar** tareas complejas con dependencias
* **Monitorear** ejecuciones en tiempo real
* **Escalar** a m√∫ltiples workers

**Caracter√≠sticas:**
* **El m√°s popular**: Est√°ndar de la industria
* **Muy maduro**: Probado en producci√≥n a gran escala
* **Gran ecosistema**: Muchos plugins y integraciones
* **Comunidad grande**: Muchos recursos y soporte

> Airflow es como un cron job inteligente con dependencias, retry y monitoreo. Es la opci√≥n segura y probada para producci√≥n.

---

## üöÄ Instalaci√≥n

> ‚ö†Ô∏è **Importante**: Antes de instalar o ejecutar cualquier comando, aseg√∫rate de activar tu entorno virtual de pyenv:
> ```bash
> pyenv activate ingenieria-de-datos
> ```
> O si usas `pyenv-virtualenv`:
> ```bash
> pyenv shell ingenieria-de-datos
> ```

```bash
# Instalaci√≥n b√°sica
pip install apache-airflow

# Con PostgreSQL (recomendado para producci√≥n)
pip install apache-airflow[postgres]

# Con providers comunes
pip install apache-airflow-providers-postgres
pip install apache-airflow-providers-aws
```

### Verificar versi√≥n instalada

Para saber qu√© versi√≥n de Airflow tienes instalada:

```bash
airflow version
```

Esto mostrar√° algo como:
```
Apache Airflow 3.1.6
```

> üí° **Importante**: La versi√≥n de Airflow determina qu√© comandos est√°n disponibles:
> - **Airflow 3.0+**: Usa `airflow standalone` (recomendado) o `airflow db migrate`
> - **Airflow 2.x**: Usa `airflow db migrate` y `airflow users create`
> - **Airflow 1.x**: Usa `airflow db init` (versiones muy antiguas)

### Configurar variables de entorno en .env

Airflow necesita algunas configuraciones. Agr√©galas a tu `.env` (ya est√°n en `.env.example`):

```bash
# Directorio donde se almacenan base de datos, logs y configuraci√≥n
# NOTA: Airflow requiere ruta absoluta. Usa $(pwd)/05_pipelines/orquestadores/.airflow al exportar
# En .env puedes usar relativa, pero al exportar debe ser absoluta
AIRFLOW_HOME=./05_pipelines/orquestadores/.airflow

# Deshabilitar DAGs de ejemplo (para ver solo tus DAGs)
AIRFLOW__CORE__LOAD_EXAMPLES=False

# Contrase√±a para el usuario admin (solo para modo tradicional)
# En modo standalone, la contrase√±a se genera autom√°ticamente
AIRFLOW_ADMIN_PASSWORD=admin
```

> ‚ö†Ô∏è **IMPORTANTE sobre rutas**: 
> - En el archivo `.env` puedes usar rutas relativas (`./05_pipelines/orquestadores/.airflow`)
> - Pero al exportar la variable, **debe ser absoluta**: `export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow`
> - O carga desde .env y convierte a absoluta: `export AIRFLOW_HOME=$(cd $(dirname .env) && pwd)/05_pipelines/orquestadores/.airflow`

**Para usar estas variables:**

1. **Carga el .env antes de ejecutar comandos de Airflow:**
   ```bash
   # Opci√≥n 1: Cargar manualmente (usar ruta absoluta)
   export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
   export AIRFLOW__CORE__LOAD_EXAMPLES=False
   export AIRFLOW_ADMIN_PASSWORD=admin
   
   # Opci√≥n 2: Cargar desde .env y convertir a absoluta
   source <(python -c "from dotenv import load_dotenv; from pathlib import Path; load_dotenv(); import os; airflow_home = os.getenv('AIRFLOW_HOME', './05_pipelines/orquestadores/.airflow'); abs_path = str(Path(airflow_home).resolve()); print(f'export AIRFLOW_HOME={abs_path}'); [print(f'export {k}={v}') for k,v in os.environ.items() if k.startswith('AIRFLOW__')]")
   ```

2. **O exporta directamente (recomendado):**
   ```bash
   export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
   export AIRFLOW__CORE__LOAD_EXAMPLES=False
   ```

> üí° **Nota**: 
> - Si no configuras `AIRFLOW_HOME`, Airflow usar√° `~/airflow` por defecto
> - `AIRFLOW__CORE__LOAD_EXAMPLES=False` deshabilita los DAGs de ejemplo (recomendado para ver solo tus DAGs)
> - `AIRFLOW_ADMIN_PASSWORD` solo se usa en el modo tradicional (Opci√≥n B)
> - En modo `standalone`, la contrase√±a se genera autom√°ticamente (no se puede configurar desde .env)
> - Si usas modo tradicional, puedes especificar la contrase√±a directamente en el comando o usar la variable del .env

### Setup inicial

> ‚ö†Ô∏è **Recuerda**: Activa tu entorno virtual antes de ejecutar:
> ```bash
> pyenv activate ingenieria-de-datos
> ```

#### Opci√≥n r√°pida: Script de configuraci√≥n autom√°tica (Recomendado)

El proyecto incluye un script que configura todo autom√°ticamente. **Esta es la forma m√°s f√°cil y r√°pida de empezar:**

```bash
# 1. Ejecuta el script de setup desde la ra√≠z del proyecto
bash 05_pipelines/ejercicios/airflow/setup-airflow.sh
```

**¬øQu√© hace el script?**
- ‚úÖ Limpia cualquier configuraci√≥n anterior de Airflow (si existe)
- ‚úÖ Crea el directorio `AIRFLOW_HOME` con ruta absoluta (`$(pwd)/05_pipelines/orquestadores/.airflow`)
- ‚úÖ Configura `AIRFLOW__CORE__LOAD_EXAMPLES=False` para ocultar DAGs de ejemplo
- ‚úÖ Crea la carpeta `dags` dentro de `AIRFLOW_HOME`
- ‚úÖ Crea symlinks de todos tus DAGs en `$AIRFLOW_HOME/dags/`
- ‚úÖ Muestra las instrucciones exactas para iniciar Airflow

**Despu√©s de ejecutar el script, inicia Airflow:**

```bash
# El script te mostrar√° la ruta exacta, pero generalmente es:
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
export AIRFLOW__CORE__LOAD_EXAMPLES=False
airflow standalone
```

> üí° **Tip**: El script muestra las instrucciones exactas al finalizar. Solo copia y pega los comandos que te muestre.

> ‚ö†Ô∏è **IMPORTANTE**: El script usa rutas absolutas autom√°ticamente, as√≠ que no tendr√°s problemas con el error "Cannot use relative path" que puede aparecer si usas rutas relativas.

#### Opci√≥n manual: Configuraci√≥n paso a paso

Si prefieres configurar todo manualmente (sin usar el script), sigue las instrucciones de abajo. **Nota**: El script autom√°tico hace todo esto por ti, as√≠ que solo necesitas esta opci√≥n si quieres entender cada paso o personalizar algo.

### Opci√≥n A: Modo Standalone (Recomendado para empezar - Airflow 3.0+)

El modo `standalone` inicia todo (webserver, scheduler, etc.) y crea un usuario admin autom√°ticamente:

> ‚ö†Ô∏è **IMPORTANTE**: Siempre configura `AIRFLOW_HOME` antes de ejecutar `airflow standalone` para que todos los archivos (incluyendo el de contrase√±as) se generen en el proyecto.

```bash
# 1. Configurar variables de entorno (OBLIGATORIO - usar ruta absoluta)
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
export AIRFLOW__CORE__LOAD_EXAMPLES=False  # Deshabilitar DAGs de ejemplo

# 2. Crear carpeta dags y symlinks de tus DAGs
mkdir -p $AIRFLOW_HOME/dags
ln -sf $(pwd)/05_pipelines/ejercicios/airflow/*.py $AIRFLOW_HOME/dags/

# 3. Verificar configuraci√≥n
echo "AIRFLOW_HOME: $AIRFLOW_HOME"
echo "DAGs configurados:"
ls -la $AIRFLOW_HOME/dags/*.py 2>/dev/null || echo "  (A√∫n no hay DAGs)"

# 4. Iniciar Airflow en modo standalone
airflow standalone
```

> ‚ö†Ô∏è **IMPORTANTE**: Airflow requiere una **ruta absoluta** para `AIRFLOW_HOME`. Usa `$(pwd)/05_pipelines/orquestadores/.airflow` en lugar de `./05_pipelines/orquestadores/.airflow`.

> üí° **Tip**: Si usaste el script `setup-airflow.sh`, ya tienes todo configurado. Solo ejecuta:
> ```bash
> export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
> export AIRFLOW__CORE__LOAD_EXAMPLES=False
> airflow standalone
> ```

> üí° **Nota**: Si ya ejecutaste `airflow standalone` sin configurar `AIRFLOW_HOME`, el archivo de contrase√±as se gener√≥ en `~/airflow/`. Puedes moverlo:
> ```bash
> # Mover el archivo de contrase√±as al directorio del proyecto
> mkdir -p ./05_pipelines/orquestadores/.airflow
> mv ~/airflow/simple_auth_manager_passwords.json.generated ./05_pipelines/orquestadores/.airflow/
> ```

Esto:
- ‚úÖ Inicializa la base de datos autom√°ticamente
- ‚úÖ Crea un usuario admin (usuario: `admin`, contrase√±a: se genera autom√°ticamente)
- ‚úÖ Inicia webserver y scheduler en un solo proceso
- ‚úÖ Abre la UI en http://localhost:8080

> ‚ö†Ô∏è **Problema com√∫n: "Invalid credentials" al iniciar sesi√≥n**
> 
> Si ves el error "401 Unauthorized" al intentar iniciar sesi√≥n, la contrase√±a se gener√≥ autom√°ticamente cuando ejecutaste `airflow standalone`.
> 
> **C√≥mo encontrar la contrase√±a:**
> 
> 1. **Si es la primera vez que ejecutas `standalone`**, la contrase√±a aparece en la terminal:
>    ```
>    =================================================================
>    Airflow is ready!
>    =================================================================
>    Login with username: admin  |  password: [AQU√ç_EST√Å_LA_CONTRASE√ëA]
>    =================================================================
>    ```
> 
> 2. **Si ya ejecutaste `standalone` antes**, la contrase√±a est√° guardada en un archivo JSON:
>    ```bash
>    # Si configuraste AIRFLOW_HOME (recomendado - en el proyecto):
>    export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
>    cat $AIRFLOW_HOME/simple_auth_manager_passwords.json.generated
>    
>    # O si no configuraste AIRFLOW_HOME (se gener√≥ en ~/airflow):
>    cat ~/airflow/simple_auth_manager_passwords.json.generated
>    ```
>    El archivo contiene algo como: `{"admin": "tu_contrase√±a_aqui"}`
>    
>    > üí° **Para mover el archivo al proyecto**: Si el archivo est√° en `~/airflow/` y quieres que est√© en el proyecto:
>    > ```bash
>    > export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
>    > mkdir -p $AIRFLOW_HOME
>    > mv ~/airflow/simple_auth_manager_passwords.json.generated $AIRFLOW_HOME/
>    > # La pr√≥xima vez, configura AIRFLOW_HOME antes de ejecutar standalone
>    > ```
> 
> 3. **O busca el archivo autom√°ticamente:**
>    ```bash
>    # Buscar el archivo de contrase√±as
>    find ~/airflow $AIRFLOW_HOME -name "*password*.json*" 2>/dev/null
>    cat $(find ~/airflow $AIRFLOW_HOME -name "*password*.json*" 2>/dev/null | head -1)
>    ```
> 
> 4. **Si no encuentras el archivo**, elimina la base de datos y reinicia:
>    ```bash
>    export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
>    rm -rf $AIRFLOW_HOME/airflow.db
>    rm -f $AIRFLOW_HOME/simple_auth_manager_passwords.json.generated
>    export AIRFLOW__CORE__LOAD_EXAMPLES=False
>    airflow standalone
>    # Ahora la contrase√±a aparecer√° en la terminal
>    ```

### Opci√≥n B: Modo tradicional (Webserver + Scheduler separados)

Si prefieres tener m√°s control, usar una contrase√±a personalizada, o usar Airflow 2.x:

```bash
# 1. Configurar AIRFLOW_HOME (si no est√° en .env)
export AIRFLOW_HOME=./05_pipelines/orquestadores/.airflow

# 2. Cargar variables del .env (opcional, para usar AIRFLOW_ADMIN_PASSWORD)
# Si tienes python-dotenv instalado:
source <(python -c "from dotenv import load_dotenv; load_dotenv(); import os; print('export AIRFLOW_ADMIN_PASSWORD=' + os.getenv('AIRFLOW_ADMIN_PASSWORD', 'admin'))")

# 3. Inicializar base de datos
airflow db migrate

# 4. Crear usuario admin con contrase√±a personalizada (Airflow 3.0+)
# En Airflow 3.0+, el comando 'users create' no existe, usa este script Python:
python << 'EOF'
from airflow.www.app import create_app
app = create_app()
with app.app_context():
    from airflow.auth.managers.fab.models import User
    import os
    password = os.getenv('AIRFLOW_ADMIN_PASSWORD', 'admin')
    if not User.find_user(username='admin'):
        User.create_user(
            username='admin',
            first_name='Admin',
            last_name='User',
            email='admin@example.com',
            role='Admin',
            password=password
        )
        print(f'‚úÖ Usuario admin creado con contrase√±a: {password}')
    else:
        print('‚ö†Ô∏è Usuario admin ya existe')
EOF

# 5. Iniciar webserver (en una terminal)
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
airflow webserver --port 8080

# 6. Iniciar scheduler (en otra terminal)
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
airflow scheduler
```

> üí° **Configurar contrase√±a en .env**:
> 1. Agrega `AIRFLOW_ADMIN_PASSWORD=tu_contrase√±a` a tu `.env` (ya est√° en `.env.example`)
> 2. Antes de crear el usuario, carga la variable: `export AIRFLOW_ADMIN_PASSWORD=$(grep AIRFLOW_ADMIN_PASSWORD .env | cut -d '=' -f2)`
> 3. O simplemente especifica la contrase√±a directamente en el comando `--password`

---

## üìÅ D√≥nde crear tus archivos

**Crea todos tus ejercicios y DAGs de Airflow en esta carpeta:**

```
05_pipelines/ejercicios/airflow/
```

### Estructura recomendada

```
05_pipelines/ejercicios/airflow/
‚îú‚îÄ‚îÄ 01-primer-dag.py          # Tu primer DAG simple
‚îú‚îÄ‚îÄ 02-dependencias.py        # DAG con dependencias complejas
‚îú‚îÄ‚îÄ 03-programacion.py       # DAG con scheduling avanzado
‚îî‚îÄ‚îÄ README.md                 # (opcional) Notas personales
```

> üí° **Importante**: Los DAGs de Airflow deben estar en la carpeta `dags/` dentro de `AIRFLOW_HOME`. Ver instrucciones abajo para configurar esto.

### C√≥mo crear un archivo

#### Opci√≥n A: Usando Cursor (Recomendado)

1. **Abre la carpeta en Cursor:**
   - En Cursor, navega a `05_pipelines/ejercicios/airflow/`
   - O usa `Cmd+P` (Mac) / `Ctrl+P` (Windows/Linux) y escribe: `ejercicios/airflow`

2. **Crea un nuevo archivo:**
   - Click derecho en la carpeta `airflow` ‚Üí "New File"
   - O usa `Cmd+N` (Mac) / `Ctrl+N` (Windows/Linux)
   - Guarda como `01-primer-dag.py` en la carpeta `airflow`

3. **Escribe tu c√≥digo** (ver ejemplos abajo)

4. **Copia el DAG a la carpeta de Airflow:**
   - Los DAGs deben estar en `AIRFLOW_HOME/dags/`
   - Puedes crear un symlink o copiar el archivo

#### Opci√≥n B: Desde terminal/Bash

1. **Navega a la carpeta de ejercicios:**
   ```bash
   cd 05_pipelines/ejercicios/airflow
   ```

2. **Crea un nuevo archivo:**
   ```bash
   touch 01-primer-dag.py
   ```

3. **Abre el archivo en Cursor o tu editor:**
   ```bash
   cursor 05_pipelines/ejercicios/airflow/01-primer-dag.py
   ```

4. **Escribe tu c√≥digo** y guarda

---

## üìä Conceptos clave

### DAG (Directed Acyclic Graph)

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'mi_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily'
)
```

### Tasks y Operators

```python
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

def mi_funcion():
    print("Ejecutando tarea")

tarea = PythonOperator(
    task_id='mi_tarea',
    python_callable=mi_funcion,
    dag=dag
)
```

---

## üéØ Primer DAG

### Paso 1: Crear el archivo

**En Cursor:**
1. Navega a `05_pipelines/ejercicios/airflow/` en el explorador de archivos
2. Click derecho ‚Üí "New File"
3. Nombra el archivo: `01-primer-dag.py`

**O desde terminal:**
```bash
cd 05_pipelines/ejercicios/airflow
touch 01-primer-dag.py
```

### Paso 2: Escribir el c√≥digo

Abre `01-primer-dag.py` en Cursor y copia este c√≥digo (o usa el archivo de ejemplo que ya est√° creado):

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd

BASE_DIR = Path(__file__).parent.parent.parent.parent

default_args = {
    'owner': 'data_engineer',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'primer_pipeline_etl',
    default_args=default_args,
    description='Primer pipeline ETL con Airflow',
    schedule_interval='@daily',
    catchup=False,
)

def extraer(**context):
    """Extrae datos del CSV de ventas."""
    ruta_entrada = BASE_DIR / '03_python' / 'data' / 'ventas.csv'
    df = pd.read_csv(ruta_entrada)
    return df.to_dict('records')

def transformar(**context):
    """Transforma los datos extra√≠dos."""
    ti = context['ti']
    datos = ti.xcom_pull(task_ids='extraer')
    df = pd.DataFrame(datos)
    df = df.dropna()
    df['total'] = df['precio'] * df['cantidad']
    return df.to_dict('records')

def cargar(**context):
    """Carga los datos transformados."""
    ti = context['ti']
    datos = ti.xcom_pull(task_ids='transformar')
    df = pd.DataFrame(datos)
    
    ruta_salida = BASE_DIR / '05_pipelines' / 'data' / 'output' / 'ventas_airflow.parquet'
    ruta_salida.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(ruta_salida, index=False)
    print(f"‚úÖ Datos guardados en {ruta_salida}")

tarea_extraer = PythonOperator(
    task_id='extraer',
    python_callable=extraer,
    dag=dag,
)

tarea_transformar = PythonOperator(
    task_id='transformar',
    python_callable=transformar,
    dag=dag,
)

tarea_cargar = PythonOperator(
    task_id='cargar',
    python_callable=cargar,
    dag=dag,
)

tarea_extraer >> tarea_transformar >> tarea_cargar
```

> üí° **Nota**: 
> - Usamos `pathlib.Path` para construir rutas de forma robusta
> - Los archivos de salida se guardan en `05_pipelines/data/output/`
> - Usamos XCom para pasar datos entre tareas (ver `ti.xcom_pull`)

### Paso 3: Configurar Airflow para usar tus DAGs

Airflow busca DAGs en `AIRFLOW_HOME/dags/`. **Es importante hacer esto antes de iniciar Airflow**:

> ‚ö†Ô∏è **IMPORTANTE**: Si ya tienes `airflow standalone` corriendo, det√©nlo (Ctrl+C), configura los DAGs, y rein√≠cialo.

**Opci√≥n A: Crear symlinks (Recomendado - los cambios se reflejan autom√°ticamente)**
```bash
# 1. Aseg√∫rate de que AIRFLOW_HOME est√© configurado (ruta absoluta)
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow

# 2. Crear carpeta dags si no existe
mkdir -p $AIRFLOW_HOME/dags

# 3. Crear symlinks de cada archivo DAG
ln -sf $(pwd)/05_pipelines/ejercicios/airflow/*.py $AIRFLOW_HOME/dags/

# 4. Verificar que los symlinks se crearon
ls -la $AIRFLOW_HOME/dags/
```

**Opci√≥n B: Copiar archivos (si prefieres copias est√°ticas)**
```bash
# Copiar DAGs a la carpeta de Airflow
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
mkdir -p $AIRFLOW_HOME/dags
cp 05_pipelines/ejercicios/airflow/*.py $AIRFLOW_HOME/dags/
```

> üí° **Nota**: Con symlinks (Opci√≥n A), cualquier cambio que hagas en los archivos originales se reflejar√° autom√°ticamente en Airflow. Con copias (Opci√≥n B), necesitar√°s copiar de nuevo despu√©s de cada cambio.

### Paso 4: Iniciar Airflow

> ‚ö†Ô∏è **Recuerda**: Activa tu entorno virtual antes de ejecutar:
> ```bash
> pyenv activate ingenieria-de-datos
> ```

**Opci√≥n A: Modo Standalone (Recomendado para Airflow 3.0+)**
```bash
# Configurar variables de entorno (usar ruta absoluta)
export AIRFLOW_HOME=$(pwd)/05_pipelines/orquestadores/.airflow
export AIRFLOW__CORE__LOAD_EXAMPLES=False  # Deshabilitar DAGs de ejemplo

airflow standalone
```

> ‚ö†Ô∏è **IMPORTANTE**: Airflow requiere una **ruta absoluta** para `AIRFLOW_HOME`. Usa `$(pwd)/05_pipelines/orquestadores/.airflow` en lugar de `./05_pipelines/orquestadores/.airflow`.

Esto inicia todo en un solo proceso. La contrase√±a del usuario admin se mostrar√° en la terminal.

> üí° **Nota**: `AIRFLOW__CORE__LOAD_EXAMPLES=False` deshabilita los 80+ DAGs de ejemplo para que veas solo los tuyos. Ya est√° configurado en `.env.example`.

**Opci√≥n B: Modo tradicional (Solo si usas Airflow 2.x o necesitas m√°s control)**

**Terminal 1 - Webserver:**
```bash
export AIRFLOW_HOME=./05_pipelines/orquestadores/.airflow
airflow webserver --port 8080
```

**Terminal 2 - Scheduler:**
```bash
export AIRFLOW_HOME=./05_pipelines/orquestadores/.airflow
airflow scheduler
```

> üí° **Nota**: Si tienes Airflow 3.0+, usa la Opci√≥n A (`standalone`). La Opci√≥n B es para versiones anteriores o cuando necesitas m√°s control sobre los procesos.

### Paso 5: Verificar que funciona

1. ‚úÖ **Abrir UI**: Ve a **http://localhost:8080**
2. ‚úÖ **Login**: Usa `admin` / contrase√±a del archivo JSON (o las credenciales que creaste)
3. ‚úÖ **Ver tus DAGs**: Deber√≠as ver `primer_pipeline_etl` y `dependencias_complejas` en la lista de DAGs

> ‚ö†Ô∏è **Si no ves tus DAGs**:
> - **Si acabas de configurar los symlinks**: Espera 30-60 segundos para que Airflow los detecte autom√°ticamente, o reinicia `airflow standalone`
> - **Verifica que los symlinks est√©n correctos**: `ls -la $AIRFLOW_HOME/dags/` debe mostrar tus archivos `.py`
> - **Revisa los logs**: En la UI, ve a "Browse" ‚Üí "Logs" para ver si hay errores en los DAGs
> - **Verifica la sintaxis**: Aseg√∫rate de que tus archivos Python no tengan errores

4. ‚úÖ **Ejecutar DAG**: Activa el toggle del DAG y luego haz click en "Trigger DAG"

> üí¨ **¬øTienes errores?** Si encuentras alg√∫n error al ejecutar tu DAG, usa el chat de Cursor (`Cmd+L` en Mac o `Ctrl+L` en Windows/Linux) para pedir ayuda. Puedes:
> - Copiar y pegar el mensaje de error completo
> - Mencionar qu√© estabas intentando hacer
> - Preguntar sobre el error espec√≠fico

---

## üîó Dependencias

```python
# Sintaxis >>
tarea_a >> tarea_b >> tarea_c

# M√∫ltiples
tarea_a >> [tarea_b, tarea_c] >> tarea_d
```

---

## üìÖ Scheduling

```python
# Diario
schedule_interval='@daily'

# Cada hora
schedule_interval='@hourly'

# Cron
schedule_interval='0 0 * * *'  # Medianoche diario
```

---

## üí° Ventajas de Airflow

### 1. Maduro y probado

* Usado en producci√≥n por miles de empresas
* Comunidad muy grande
* Muchos recursos disponibles

### 2. Gran ecosistema

* Muchos providers (AWS, GCP, Azure, etc.)
* Plugins para casi todo
* Integraciones con servicios comunes

### 3. UI completa

* Monitoreo en tiempo real
* Logs detallados
* Visualizaci√≥n de DAGs
* Gesti√≥n de conexiones y variables

---

## ‚ö†Ô∏è Desventajas

### 1. Curva de aprendizaje

* Conceptos nuevos (DAGs, Operators)
* Configuraci√≥n inicial m√°s compleja
* Requiere entender scheduling

### 2. Overhead

* Requiere base de datos
* Necesita scheduler corriendo
* M√°s recursos que soluciones simples

---

## üéØ Cu√°ndo usar Airflow

‚úÖ **Usa Airflow cuando:**
* Necesitas orquestaci√≥n compleja
* Tienes m√∫ltiples pipelines
* Necesitas programaci√≥n avanzada
* Quieres est√°ndar de industria

‚ùå **No uses Airflow cuando:**
* Pipeline muy simple
* Solo necesitas ejecutar ocasionalmente
* No tienes infraestructura para gestionarlo

---

## üöÄ Alternativas gestionadas

Si no quieres gestionar Airflow:

* **Google Cloud Composer**: Airflow gestionado en GCP
* **AWS MWAA**: Airflow gestionado en AWS
* **Astronomer**: Plataforma gestionada de Airflow

---

## üéØ Ejercicios pr√°cticos

Crea estos archivos en `05_pipelines/ejercicios/airflow/`:

### Ejercicio 1: Primer DAG
**Archivo:** `01-primer-dag.py`
- Crea un DAG simple con 3 tareas: extraer, transformar, cargar
- Usa el ejemplo de arriba como base
- Ejecuta el DAG desde la UI de Airflow

### Ejercicio 2: Dependencias complejas
**Archivo:** `02-dependencias.py`
- Crea tareas que se ejecuten en paralelo
- Crea tareas que dependan de m√∫ltiples tareas anteriores
- Observa c√≥mo Airflow maneja las dependencias

### Ejercicio 3: Scheduling
- Modifica `schedule_interval` en tus DAGs
- Prueba diferentes frecuencias: `@daily`, `@hourly`, `0 0 * * *`
- Observa c√≥mo Airflow programa las ejecuciones

### Ejercicio 4: UI
- Explora la UI de Airflow
- Revisa logs de tareas ejecutadas
- Visualiza el gr√°fico de dependencias
- Monitorea ejecuciones en tiempo real

---

## üöÄ Pr√≥ximos pasos

* **Operators avanzados**: DockerOperator, KubernetesPodOperator
* **XComs**: Pasar datos entre tareas
* **Hooks**: Conectar con servicios externos
* **Plugins**: Crear funcionalidad custom

---

> **Recuerda**: Airflow es poderoso pero tiene overhead. √ösalo cuando necesites sus capacidades avanzadas. Para empezar, considera primero **[Prefect](prefect.md)** o **[Dagster](dagster.md)** que son m√°s simples.
