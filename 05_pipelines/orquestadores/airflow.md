# Apache Airflow: El Est√°ndar de la Industria

Apache Airflow es el orquestador de workflows m√°s popular para Data Engineering. Aprende los conceptos b√°sicos y c√≥mo usarlo.

---

## üß† ¬øQu√© es Airflow?

Apache Airflow es una plataforma para:
* **Programar** workflows de datos
* **Orquestar** tareas complejas con dependencias
* **Monitorear** ejecuciones en tiempo real
* **Escalar** a m√∫ltiples workers

**Caracter√≠sticas:**
* **El m√°s popular**: Est√°ndar de la industria
* **Muy maduro**: Probado en producci√≥n a gran escala
* **Gran ecosistema**: Muchos plugins y integraciones
* **Comunidad grande**: Muchos recursos y soporte

> Airflow es como un cron job inteligente con dependencias, retry y monitoreo. Es la opci√≥n segura y probada para producci√≥n.

---

## üöÄ Instalaci√≥n

```bash
# Instalaci√≥n b√°sica
pip install apache-airflow

# Con PostgreSQL
pip install apache-airflow[postgres]

# Con providers comunes
pip install apache-airflow-providers-postgres
pip install apache-airflow-providers-aws
```

### Setup inicial

```bash
# Inicializar base de datos
airflow db init

# Crear usuario admin
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin

# Iniciar webserver
airflow webserver --port 8080

# Iniciar scheduler (en otra terminal)
airflow scheduler
```

---

## üìä Conceptos clave

### DAG (Directed Acyclic Graph)

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'mi_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule_interval='@daily'
)
```

### Tasks y Operators

```python
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

def mi_funcion():
    print("Ejecutando tarea")

tarea = PythonOperator(
    task_id='mi_tarea',
    python_callable=mi_funcion,
    dag=dag
)
```

---

## üéØ Ejemplo completo

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def extraer():
    print("Extrayendo datos...")

def transformar():
    print("Transformando datos...")

def cargar():
    print("Cargando datos...")

default_args = {
    'owner': 'data_engineer',
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'pipeline_etl',
    default_args=default_args,
    description='Pipeline ETL',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False
)

tarea_extraer = PythonOperator(
    task_id='extraer',
    python_callable=extraer,
    dag=dag
)

tarea_transformar = PythonOperator(
    task_id='transformar',
    python_callable=transformar,
    dag=dag
)

tarea_cargar = PythonOperator(
    task_id='cargar',
    python_callable=cargar,
    dag=dag
)

tarea_extraer >> tarea_transformar >> tarea_cargar
```

---

## üîó Dependencias

```python
# Sintaxis >>
tarea_a >> tarea_b >> tarea_c

# M√∫ltiples
tarea_a >> [tarea_b, tarea_c] >> tarea_d
```

---

## üìÖ Scheduling

```python
# Diario
schedule_interval='@daily'

# Cada hora
schedule_interval='@hourly'

# Cron
schedule_interval='0 0 * * *'  # Medianoche diario
```

---

## üí° Ventajas de Airflow

### 1. Maduro y probado

* Usado en producci√≥n por miles de empresas
* Comunidad muy grande
* Muchos recursos disponibles

### 2. Gran ecosistema

* Muchos providers (AWS, GCP, Azure, etc.)
* Plugins para casi todo
* Integraciones con servicios comunes

### 3. UI completa

* Monitoreo en tiempo real
* Logs detallados
* Visualizaci√≥n de DAGs
* Gesti√≥n de conexiones y variables

---

## ‚ö†Ô∏è Desventajas

### 1. Curva de aprendizaje

* Conceptos nuevos (DAGs, Operators)
* Configuraci√≥n inicial m√°s compleja
* Requiere entender scheduling

### 2. Overhead

* Requiere base de datos
* Necesita scheduler corriendo
* M√°s recursos que soluciones simples

---

## üéØ Cu√°ndo usar Airflow

‚úÖ **Usa Airflow cuando:**
* Necesitas orquestaci√≥n compleja
* Tienes m√∫ltiples pipelines
* Necesitas programaci√≥n avanzada
* Quieres est√°ndar de industria

‚ùå **No uses Airflow cuando:**
* Pipeline muy simple
* Solo necesitas ejecutar ocasionalmente
* No tienes infraestructura para gestionarlo

---

## üöÄ Alternativas gestionadas

Si no quieres gestionar Airflow:

* **Google Cloud Composer**: Airflow gestionado en GCP
* **AWS MWAA**: Airflow gestionado en AWS
* **Astronomer**: Plataforma gestionada de Airflow

---

## üéØ Ejercicios

1. Instala Airflow localmente
2. Crea tu primer DAG
3. Configura dependencias entre tareas
4. Explora la UI de Airflow

---

## üéØ Ejercicios

1. Instala Airflow localmente
2. Crea tu primer DAG
3. Configura dependencias entre tareas
4. Explora la UI de Airflow

---

## üöÄ Pr√≥ximos pasos

* **Operators avanzados**: DockerOperator, KubernetesPodOperator
* **XComs**: Pasar datos entre tareas
* **Hooks**: Conectar con servicios externos
* **Plugins**: Crear funcionalidad custom

---

> **Recuerda**: Airflow es poderoso pero tiene overhead. √ösalo cuando necesites sus capacidades avanzadas. Para empezar, considera primero **[Prefect](prefect.md)** o **[Dagster](dagster.md)** que son m√°s simples.
