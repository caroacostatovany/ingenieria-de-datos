# Proyecto 1: Pipeline ETL Simple

Construye tu primer pipeline ETL completo: extrae datos de CSV, transforma y limpia, y carga a archivos de salida.

> âœ… **Este proyecto incluye cÃ³digo funcional y un dataset de ejemplo listo para usar**

---

## ğŸ¯ Objetivo

Aprender los fundamentos de un pipeline ETL:
* **Extract**: Leer datos de archivos CSV
* **Transform**: Limpiar y transformar datos
* **Load**: Guardar datos transformados en diferentes formatos

---

## ğŸ“‹ Requisitos previos

* Python 3.8+
* Conocimientos bÃ¡sicos de Python y Pandas

---

## ğŸš€ Inicio RÃ¡pido

### 1. Preparar entorno

```bash
# Navegar al proyecto
cd 07_proyectos/principiante/proyecto_01_etl_simple

# Crear entorno virtual (opcional pero recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Ejecutar el pipeline

```bash
# Ejecutar el pipeline
python pipeline.py
```

**Â¡Eso es todo!** El pipeline:
- âœ… Lee el archivo `data/ventas.csv`
- âœ… Transforma y limpia los datos
- âœ… Guarda los resultados en `output/`

---

## ğŸ“ Estructura del proyecto

```
proyecto_01_etl_simple/
â”œâ”€â”€ README.md                 # Este archivo
â”œâ”€â”€ requirements.txt          # Dependencias Python
â”œâ”€â”€ pipeline.py              # Pipeline ETL completo y funcional
â”œâ”€â”€ data/
â”‚   â””â”€â”€ ventas.csv          # Dataset de ejemplo (15 registros)
â””â”€â”€ output/                  # Directorio de salida (se crea automÃ¡ticamente)
    â”œâ”€â”€ ventas_transformadas_YYYYMMDD_HHMMSS.csv
    â”œâ”€â”€ ventas_transformadas_YYYYMMDD_HHMMSS.parquet
    â””â”€â”€ resumen_ventas_YYYYMMDD_HHMMSS.csv
```

---

## ğŸ“Š Dataset de ejemplo

El archivo `data/ventas.csv` contiene datos de ventas con las siguientes columnas:

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| fecha | string | Fecha de la venta (YYYY-MM-DD) |
| producto | string | Nombre del producto |
| cantidad | int | Cantidad vendida |
| precio | float | Precio unitario |
| cliente | string | Nombre del cliente |
| ciudad | string | Ciudad donde se realizÃ³ la venta |

**Ejemplo de datos:**
```csv
fecha,producto,cantidad,precio,cliente,ciudad
2024-01-15,Producto A,5,10.50,Cliente 1,Madrid
2024-01-16,Producto B,3,25.00,Cliente 2,Barcelona
...
```

---

## ğŸ” Â¿QuÃ© hace el pipeline?

### Extract (ExtracciÃ³n)
- Lee el archivo CSV `data/ventas.csv`
- Valida que el archivo existe
- Muestra estadÃ­sticas bÃ¡sicas

### Transform (TransformaciÃ³n)
1. **Convierte fechas** a formato datetime
2. **Calcula total** por venta (precio Ã— cantidad)
3. **Agrega columna de mes** para anÃ¡lisis
4. **Elimina duplicados**
5. **Elimina registros con valores nulos** en columnas crÃ­ticas
6. **Filtra registros vÃ¡lidos** (total > 0)
7. **Ordena por fecha**

### Load (Carga)
Guarda los datos en 3 archivos:
1. **CSV**: `ventas_transformadas_*.csv` - Formato legible
2. **Parquet**: `ventas_transformadas_*.parquet` - Formato eficiente
3. **Resumen**: `resumen_ventas_*.csv` - Agregaciones por producto

---

## ğŸ“ Ejemplo de salida

DespuÃ©s de ejecutar el pipeline, verÃ¡s algo como:

```
============================================================
ğŸš€ Pipeline ETL Simple - Ejecutando...
============================================================
ğŸ“¥ Extrayendo datos de data/ventas.csv...
âœ… ExtraÃ­dos 15 registros
   Columnas: fecha, producto, cantidad, precio, cliente, ciudad
ğŸ”„ Transformando datos...
âœ… TransformaciÃ³n completada: 15 registros vÃ¡lidos
   Total de ventas: â‚¬450.00
ğŸ’¾ Guardando datos transformados...
   âœ… CSV guardado: output/ventas_transformadas_20240115_143022.csv
   âœ… Parquet guardado: output/ventas_transformadas_20240115_143022.parquet
   âœ… Resumen guardado: output/resumen_ventas_20240115_143022.csv
âœ… Pipeline completado exitosamente!

============================================================
âœ¨ Pipeline ejecutado exitosamente!
ğŸ“ Archivos de salida en: output
============================================================
```

---

## ğŸ§ª Experimentar con el cÃ³digo

### Modificar el dataset

1. Edita `data/ventas.csv` y agrega mÃ¡s registros
2. Ejecuta el pipeline nuevamente: `python pipeline.py`
3. Observa cÃ³mo se procesan los nuevos datos

### Agregar nuevas transformaciones

Edita `pipeline.py` en la funciÃ³n `transform()`:

```python
def transform(df: pd.DataFrame) -> pd.DataFrame:
    # ... cÃ³digo existente ...
    
    # Agregar nueva transformaciÃ³n
    df_transformed['descuento'] = df_transformed['total'] * 0.1  # 10% descuento
    
    return df_transformed
```

### Cambiar el formato de salida

Modifica la funciÃ³n `load()` para guardar en otros formatos:

```python
# Guardar en JSON
df.to_json(output_dir / 'ventas.json', orient='records', indent=2)

# Guardar en Excel
df.to_excel(output_dir / 'ventas.xlsx', index=False)
```

---

## ğŸ“š Conceptos aprendidos

Al completar este proyecto, habrÃ¡s aprendido:

âœ… **Extract**: CÃ³mo leer datos de archivos CSV con Pandas  
âœ… **Transform**: TÃ©cnicas de limpieza y transformaciÃ³n de datos  
âœ… **Load**: CÃ³mo guardar datos en diferentes formatos  
âœ… **Manejo de errores**: ValidaciÃ³n de archivos y datos  
âœ… **Estructura de proyectos**: OrganizaciÃ³n de cÃ³digo en funciones  
âœ… **Logging**: Mensajes informativos durante la ejecuciÃ³n  

---

## ğŸ”— PrÃ³ximos pasos

Una vez que domines este pipeline bÃ¡sico:

1. **Proyecto 2**: [AnÃ¡lisis de Datos con Pandas](../proyecto_02_analisis_pandas/README.md)
   - AnÃ¡lisis exploratorio de datos (EDA)
   - Visualizaciones con Matplotlib/Seaborn

2. **Proyecto 3**: [Pipeline con Docker](../proyecto_03_docker_pipeline/README.md)
   - Containerizar el pipeline
   - Ejecutar en contenedores Docker

3. **MÃ³dulo SQL**: [02_sql](../../../02_sql/README.md)
   - Aprender a cargar datos a bases de datos
   - Usar SQL para transformaciones

---

## ğŸ› SoluciÃ³n de problemas

### Error: "No se encontrÃ³ el archivo"

**SoluciÃ³n**: AsegÃºrate de ejecutar el script desde el directorio del proyecto:
```bash
cd 07_proyectos/principiante/proyecto_01_etl_simple
python pipeline.py
```

### Error: "ModuleNotFoundError: No module named 'pandas'"

**SoluciÃ³n**: Instala las dependencias:
```bash
pip install -r requirements.txt
```

### Error al guardar Parquet

**SoluciÃ³n**: Instala pyarrow:
```bash
pip install pyarrow
```

---

## ğŸ’¡ Tips

- **Revisa los archivos de salida** para entender quÃ© datos se generaron
- **Experimenta modificando el cÃ³digo** para ver cÃ³mo cambian los resultados
- **Agrega mÃ¡s datos** al CSV para probar con datasets mÃ¡s grandes
- **Lee el cÃ³digo lÃ­nea por lÃ­nea** para entender cada transformaciÃ³n

---

## ğŸ“– Recursos adicionales

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Python para Data Engineers](../../../03_python/fundamentos/fundamentos-python.md)
- [Manejo de Archivos](../../../03_python/fundamentos/manejo-de-archivos.md)
- [Â¿QuÃ© es un Pipeline?](../../../01_fundamentos/01_que-es-un-pipeline.md)

---

> **ğŸ’¡ Recuerda**: Este es un ejemplo educativo. En producciÃ³n, agrega mÃ¡s validaciones, logging estructurado, y manejo robusto de errores.
