# Roadmap para convertirte en Data Engineer

Este roadmap estÃ¡ diseÃ±ado para ayudarte a **convertirte en Data Engineer de forma progresiva**, construyendo **bases sÃ³lidas** antes de avanzar a sistemas mÃ¡s complejos.

No es una lista de herramientas.
Es una **forma de pensar**.

---

## ğŸ§­ Principios del roadmap

Antes de empezar, algunas reglas importantes:

* No necesitas aprender todo a la vez
* Los fundamentos importan mÃ¡s que las herramientas
* Entender *por quÃ©* es mÃ¡s importante que el *cÃ³mo*
* La experiencia se construye paso a paso

---

## ğŸŸ¢ Etapa 0 â€” Fundamentos generales

**Objetivo:** Entender el ecosistema de datos.

Aprende:

* QuÃ© es Data Engineering
* CÃ³mo fluye un dato desde su origen hasta el negocio
* Diferencia entre:

  * Data Engineer
  * Data Analyst
  * Data Scientist

No necesitas escribir cÃ³digo todavÃ­a.
Necesitas **contexto**.

ğŸ“ Contenido recomendado:

* [Â¿QuÃ© es Data Engineering?](que-es-data-engineering.md)
* [Roles en Datos](roles-en-datos.md)

---

## ğŸŸ¢ Etapa 0.5 â€” Cursor: Tu Copiloto de AI (PRIMERO)

**Objetivo:** Configurar Cursor para usar AI como copiloto desde el inicio.

**âš ï¸ IMPORTANTE:** Configura Cursor **ANTES** de empezar con SQL o Python. Te ayudarÃ¡ durante todo el aprendizaje.

Aprende:

* **Instalar y configurar Cursor**
* **Clonar este repositorio en Cursor**
* **Usar el chat de AI** para hacer preguntas sobre el contenido
* **Pedir explicaciones** adaptadas a tu nivel
* **Solicitar ayuda** para ejecutar comandos (Docker, etc.)

> ğŸ’¡ **Tip**: Usa Cursor desde el dÃ­a 1. Puedes preguntarle sobre cualquier archivo del repositorio, pedir explicaciones simples, o solicitar ayuda para ejecutar comandos.

ğŸ“ Contenido recomendado:

* **[Cursor para Data Engineers](../06_inteligencia_artificial/herramientas/cursor-para-data-engineers.md)** â­ **EMPIEZA AQUÃ**

---

## ğŸŸ¢ Etapa 0.6 â€” Herramientas esenciales

**Objetivo:** Configurar tu entorno de trabajo.

Aprende:

* **Git y GitHub** para versionar cÃ³digo
* **Archivos .env** para gestionar configuraciones
* **Docker** para entornos reproducibles

Estas herramientas te acompaÃ±arÃ¡n durante todo el camino.
Aprenderlas temprano te ahorrarÃ¡ tiempo despuÃ©s.

ğŸ“ Contenido recomendado:

* [Tipos de Datos](../01_fundamentos/00_tipos-de-datos.md)
* [Â¿QuÃ© es un Pipeline?](../01_fundamentos/01_que-es-un-pipeline.md)
* [Batch vs Streaming](../01_fundamentos/02_batch-vs-streaming.md)
* [Git y GitHub para Data Engineers](../01_fundamentos/03_git-y-github-para-data-engineers.md)
* [Archivos .env para Data Engineers](../01_fundamentos/04_archivos-env-para-data-engineers.md)
* [Docker para Data Engineers](../01_fundamentos/05_docker-para-data-engineers.md)

---

## ğŸŸ¡ Etapa 1 â€” SQL (la base de todo)

**Objetivo:** Poder consultar y transformar datos con confianza.

Aprende:

* **Conceptos fundamentales**: SQL transaccional vs analÃ­tico
* **BÃ¡sico**: SELECT, WHERE, JOIN, GROUP BY
* **Intermedio**: Subqueries, CTEs, Window functions
* **Avanzado**: OptimizaciÃ³n, particionamiento, Ã­ndices
* **Modelado relacional**: DiseÃ±o de esquemas, normalizaciÃ³n

Buenas prÃ¡cticas:

* Queries legibles
* Nombres claros
* Evitar lÃ³gica innecesaria
* OptimizaciÃ³n para grandes volÃºmenes

ğŸ‘‰ Si sabes SQL, siempre tendrÃ¡s trabajo en datos.

ğŸ“ Contenido recomendado:

* [IntroducciÃ³n a SQL](../01_fundamentos/06_introduccion-sql.md)
* [SQL bÃ¡sico](../02_sql/sql-basico/) *(prÃ³ximo)*
* [SQL intermedio](../02_sql/sql-intermedio/) *(prÃ³ximo)*
* [SQL avanzado](../02_sql/sql-avanzado/) *(prÃ³ximo)*
* [Modelado Relacional](../02_sql/modelado-relacional.md) *(prÃ³ximo)*
* [Base de datos local con Docker](../02_sql/README-DOCKER.md) - Para practicar

---

## ğŸŸ¡ Etapa 2 â€” Python para Data Engineering

**Objetivo:** Automatizar y estructurar procesos.

Aprende:

* **Fundamentos Python** para Data Engineering
* **Manejo de archivos**: CSV, JSON, Parquet
* **Pandas** para manipulaciÃ³n de datos
* **Scripts vs mÃ³dulos**: Estructura de proyectos
* **Manejo de errores** y logging
* **IntegraciÃ³n con SQL** y bases de datos

No se trata de "saber todo Python".
Se trata de **escribir cÃ³digo mantenible**.

ğŸ“ Contenido recomendado:

* [Fundamentos Python](../03_python/fundamentos/)
* [Pandas para Datos](../03_python/pandas/)
* [Storytelling con Datos](../03_python/storytelling/)
* [Ejemplos](../03_python/ejemplos/)

---

## ğŸŸ  Etapa 3 â€” Modelado y calidad de datos

**Objetivo:** Que los datos sean confiables.

Aprende:

* **Modelado analÃ­tico**: Star Schema, Snowflake, tablas de hechos y dimensiones
* **Calidad de datos**: Dimensiones de calidad, mÃ©tricas, KPIs
* **Validaciones**: Checks de integridad, validaciÃ³n de esquemas
* **Testing de datos**: Tests unitarios, tests de integraciÃ³n
* **DetecciÃ³n de errores**: Alertas y notificaciones

AquÃ­ pasas de "mover datos" a **ingenierÃ­a real**.

ğŸ“ Contenido recomendado:

* [Modelado analÃ­tico](../04_modelado_y_calidad/modelado/)
* [Calidad de datos](../04_modelado_y_calidad/calidad/)
* [Validaciones](../04_modelado_y_calidad/calidad/validaciones/)
* [Testing de datos](../04_modelado_y_calidad/calidad/validaciones/testing-de-datos.md)
* [Herramientas](../04_modelado_y_calidad/calidad/herramientas/)
* [Ejemplos (Notebooks)](../04_modelado_y_calidad/ejemplos/)

---

## ğŸŸ  Etapa 4 â€” Pipelines y orquestaciÃ³n

**Objetivo:** Automatizar procesos de forma robusta.

Aprende:

* **Conceptos**: QuÃ© es un pipeline, diferencia con scripts
* **Componentes**: Tareas, dependencias, monitoreo
* **Batch vs Streaming**: CuÃ¡ndo usar cada enfoque
* **Pipelines con Python**: Construir pipelines desde cero
* **Orquestadores**: IntroducciÃ³n a Airflow
* **Buenas prÃ¡cticas**: Manejo de errores, logging, testing

El foco no es la herramienta.
Es la **orquestaciÃ³n correcta**.

ğŸ“ Contenido recomendado:

* [Â¿QuÃ© es un Pipeline?](../05_pipelines/pipelines-basicos/que-es-un-pipeline.md) *(conceptual)*
* [Batch vs Streaming](../01_fundamentos/02_batch-vs-streaming.md)
* [Pipelines bÃ¡sicos](../05_pipelines/pipelines-basicos/)
* [Pipelines con Python](../05_pipelines/pipelines-basicos/pipelines-con-python.md)
* [IntroducciÃ³n a Airflow](../05_pipelines/orquestadores/airflow.md)
* [Buenas PrÃ¡cticas](../01_fundamentos/07_buenas-practicas.md)

---

## ğŸ¤– Etapa 5 â€” AI como copiloto

**Objetivo:** Aumentar productividad sin perder criterio.

Aprende a usar AI para:

* **Entender cÃ³digo**: Explicar funciones complejas, SQL, pipelines
* **Generar cÃ³digo**: SQL queries, funciones Python, pipelines
* **Documentar**: Docstrings, READMEs, documentaciÃ³n tÃ©cnica
* **Generar tests**: Tests unitarios, tests de integraciÃ³n
* **Debugging**: Identificar errores, sugerir soluciones
* **Refactorizar**: Mejorar cÃ³digo existente

Pero tambiÃ©n aprende:

* **CuÃ¡ndo NO usar AI**: Decisiones crÃ­ticas, validaciones importantes
* **CÃ³mo validar resultados**: Revisar siempre el cÃ³digo generado
* **LÃ­mites de la AI**: QuÃ© puede y quÃ© no puede hacer

La AI es una herramienta.
La responsabilidad sigue siendo tuya.

ğŸ“ Contenido recomendado:

* [Cursor para Data Engineers](../06_inteligencia_artificial/herramientas/cursor-para-data-engineers.md)
* [CÃ³mo usar AI como DE](../06_inteligencia_artificial/uso-practico/como-usar-ai-como-de.md)
* [Ejemplos de Prompts](../06_inteligencia_artificial/uso-practico/ejemplos-prompts.md)
* [DocumentaciÃ³n con AI](../06_inteligencia_artificial/uso-practico/documentacion-con-ai.md)
* [LÃ­mites de la AI](../06_inteligencia_artificial/limites-de-la-ai.md)
* [Buenas PrÃ¡cticas de AI](../06_inteligencia_artificial/buenas-practicas-ai.md)

---

## ğŸ”µ Etapa 6 â€” Data Engineering en la Nube

**Objetivo:** Aplicar conocimientos en entornos cloud.

Aprende:

* **Conceptos fundamentales**: Serverless, almacenamiento de objetos, servicios gestionados
* **Proveedores principales**: AWS, GCP, Azure
* **Servicios clave**: Almacenamiento, procesamiento, orquestaciÃ³n
* **Costos y optimizaciÃ³n**: Free tier, monitoreo de costos
* **Arquitecturas cloud**: Data Warehouse vs Data Lake en cloud

> ğŸ’¡ **Nota**: Puedes aprender cloud en paralelo con otras etapas. No es necesario esperar hasta aquÃ­.

ğŸ“ Contenido recomendado:

* [Data Engineering en la Nube](../01_fundamentos/08_data-engineering-en-la-nube.md)
* [AWS](../08_cloud/aws/) *(prÃ³ximo)*
* [Google Cloud Platform](../08_cloud/gcp/) *(prÃ³ximo)*
* [Microsoft Azure](../08_cloud/azure/) *(prÃ³ximo)*
* [Multi-Cloud](../08_cloud/multi-cloud/) *(prÃ³ximo)*

---

## ğŸš€ Etapa 7 â€” Proyectos end-to-end

**Objetivo:** Integrar todo lo aprendido.

Construye proyectos que incluyan:

* **Ingesta**: Extraer datos de fuentes (APIs, bases de datos, archivos)
* **TransformaciÃ³n**: Limpiar, normalizar, enriquecer datos
* **Modelado**: DiseÃ±ar esquemas analÃ­ticos apropiados
* **ValidaciÃ³n**: Tests de calidad, checks de integridad
* **OrquestaciÃ³n**: Pipelines automatizados y monitoreados
* **DocumentaciÃ³n**: READMEs, comentarios, guÃ­as de uso

Un proyecto bien hecho vale mÃ¡s que 10 cursos.

ğŸ“ Contenido recomendado:

* [Proyectos Principiantes](../07_proyectos/principiante/)
* [Proyectos Intermedios](../07_proyectos/intermedio/)
* [Proyectos Avanzados](../07_proyectos/avanzado/)

---

## ğŸ§  Â¿CuÃ¡nto tiempo toma este roadmap?

Depende de:

* tu punto de partida
* tu constancia
* tu contexto profesional

Como referencia:

* 3â€“6 meses para bases sÃ³lidas
* 6â€“12 meses para nivel intermedio
* aprendizaje continuo para nivel senior

No hay atajos reales.

---

## â¡ï¸ Â¿QuÃ© sigue despuÃ©s?

Una vez domines este roadmap, el siguiente paso natural es aplicar todo en un **Data Lake real**, donde:

* los datos escalan
* los errores cuestan
* las decisiones importan

ğŸ‘‰ Repositorio complementario:
`data-lake-engineering-en-espanol` (prÃ³ximamente)

---

**La IngenierÃ­a de Datos no se aprende en lÃ­nea recta.
Se construye con criterio, prÃ¡ctica y paciencia.**
