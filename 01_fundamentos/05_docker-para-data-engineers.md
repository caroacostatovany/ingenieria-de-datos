# Docker para Data Engineers

Docker es una herramienta que permite **empaquetar aplicaciones y sus dependencias en contenedores**, creando entornos consistentes y reproducibles. Para Data Engineers, Docker es esencial para crear pipelines portables y entornos de desarrollo aislados.

> üí° **No te preocupes**: Este documento es **solo te√≥rico**. No necesitas configurar ni ejecutar nada ahora. Solo lee para entender qu√© es Docker y c√≥mo se usa en Data Engineering. Cuando llegue el momento de practicar, tendr√°s gu√≠as paso a paso.

---

## üß† ¬øPor qu√© Docker es importante en Data Engineering?

En Data Engineering, Docker resuelve problemas comunes:

### Problema: "Funciona en mi m√°quina"
* Diferentes versiones de Python
* Librer√≠as instaladas globalmente
* Configuraciones del sistema diferentes
* Dependencias conflictivas

### Soluci√≥n: Contenedores Docker
* **Entorno consistente** en desarrollo, testing y producci√≥n
* **Aislamiento** de dependencias entre proyectos
* **Reproducibilidad** garantizada
* **Portabilidad** entre m√°quinas y servidores

> Docker hace que tu pipeline funcione igual en tu laptop, en CI/CD y en producci√≥n.

---

## üîÅ Conceptos fundamentales

### Imagen (Image)
Un **template** que define qu√© contiene tu aplicaci√≥n:
* Sistema operativo base
* Librer√≠as y dependencias
* C√≥digo de tu aplicaci√≥n
* Configuraciones

### Contenedor (Container)
Una **instancia ejecut√°ndose** de una imagen:
* Aislado del sistema host
* Tiene su propio sistema de archivos
* Puede tener red propia
* Se puede iniciar, detener y eliminar

### Dockerfile
Un **archivo de texto** que describe c√≥mo construir una imagen:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "pipeline.py"]
```

---

## üìä Docker en el flujo de Data Engineering

### Desarrollo local
```bash
# Construir imagen
docker build -t mi-pipeline .

# Ejecutar pipeline
docker run mi-pipeline
```

### Testing
```bash
# Ejecutar tests en contenedor
docker run mi-pipeline pytest tests/
```

### Producci√≥n
```bash
# Desplegar en servidor
docker run -d --name pipeline-prod mi-pipeline
```

---

## üõ†Ô∏è Casos de uso comunes

### 1. Pipeline Python aislado

**Dockerfile:**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalar dependencias
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo
COPY src/ ./src/
COPY pipeline.py .

# Ejecutar pipeline
CMD ["python", "pipeline.py"]
```

**Uso:**
```bash
docker build -t data-pipeline .
docker run data-pipeline
```

### 2. Base de datos local para desarrollo

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: mydata
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

**Uso:**
```bash
docker-compose up -d  # Iniciar base de datos
docker-compose down   # Detener base de datos
```

### 3. Entorno de desarrollo completo

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  pipeline:
    build: .
    volumes:
      - ./data:/app/data
      - ./src:/app/src
    depends_on:
      - postgres
  
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: analytics
    ports:
      - "5432:5432"
  
  jupyter:
    image: jupyter/scipy-notebook
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
```

---

## üéØ Comandos Docker esenciales

### Construir y ejecutar
```bash
# Construir imagen
docker build -t nombre-imagen .

# Ejecutar contenedor
docker run nombre-imagen

# Ejecutar con vol√∫menes (montar carpetas)
docker run -v $(pwd)/data:/app/data nombre-imagen

# Ejecutar en background
docker run -d nombre-imagen
```

### Gesti√≥n de contenedores
```bash
# Ver contenedores corriendo
docker ps

# Ver todos los contenedores
docker ps -a

# Detener contenedor
docker stop nombre-contenedor

# Eliminar contenedor
docker rm nombre-contenedor

# Ver logs
docker logs nombre-contenedor
```

### Gesti√≥n de im√°genes
```bash
# Listar im√°genes
docker images

# Eliminar imagen
docker rmi nombre-imagen

# Ver historial de imagen
docker history nombre-imagen
```

### Docker Compose
```bash
# Iniciar servicios
docker-compose up

# Iniciar en background
docker-compose up -d

# Detener servicios
docker-compose down

# Ver logs
docker-compose logs

# Reconstruir im√°genes
docker-compose build
```

---

## üí° Buenas pr√°cticas para Data Engineers

### 1. Usa im√°genes base peque√±as
```dockerfile
# ‚ùå Evita
FROM python:3.11

# ‚úÖ Mejor
FROM python:3.11-slim
```

### 2. Cachea dependencias
```dockerfile
# Copiar requirements primero (capa que cambia menos)
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copiar c√≥digo despu√©s (capa que cambia m√°s)
COPY . .
```

### 3. No incluyas datos en la imagen
```dockerfile
# ‚ùå No hagas esto
COPY data/ ./data/

# ‚úÖ Usa vol√∫menes en runtime
docker run -v $(pwd)/data:/app/data mi-pipeline
```

### 4. Usa .dockerignore
Crea un archivo `.dockerignore`:
```
__pycache__/
*.pyc
.env
data/
.git/
*.md
```

### 5. Variables de entorno para configuraci√≥n
```dockerfile
ENV DB_HOST=localhost
ENV DB_PORT=5432
```

```bash
docker run -e DB_HOST=prod-server mi-pipeline
```

---

## üîó Docker en pipelines de datos

### Pipeline con Airflow
```dockerfile
FROM apache/airflow:2.7.0

USER root
RUN pip install pandas sqlalchemy

USER airflow
COPY dags/ /opt/airflow/dags/
```

### Pipeline con Python + dependencias
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Instalar dependencias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo
COPY . .

# Ejecutar
CMD ["python", "main.py"]
```

---

## üöÄ Integraci√≥n con CI/CD

Docker es perfecto para CI/CD:

```yaml
# .github/workflows/pipeline.yml
name: Run Pipeline

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build image
        run: docker build -t pipeline .
      - name: Run tests
        run: docker run pipeline pytest
      - name: Run pipeline
        run: docker run pipeline python main.py
```

---

## ‚ö†Ô∏è Consideraciones importantes

### Vol√∫menes para datos
* **No** incluyas datos grandes en im√°genes
* **Usa vol√∫menes** para montar datos en runtime
* **Considera** vol√∫menes nombrados para persistencia

### Recursos
* Docker consume memoria y CPU
* Ajusta l√≠mites si es necesario:
  ```bash
  docker run --memory="512m" --cpus="1.0" mi-pipeline
  ```

### Seguridad
* No incluyas credenciales en im√°genes
* Usa variables de entorno o secretos
* Escanea im√°genes por vulnerabilidades

---

## ‚û°Ô∏è ¬øQu√© sigue?

Para continuar:
1. **Revisa [SETUP.md](../../SETUP.md)** y ejecuta la configuraci√≥n si a√∫n no lo has hecho
   - Incluye instrucciones para instalar Docker
   - Configuraci√≥n de base de datos local con Docker Compose
   - Todo lo necesario para empezar a practicar
2. **[Introducci√≥n a SQL](06_introduccion-sql.md)** - Lenguaje para trabajar con datos estructurados

Para investigar:
1. **Aprende Docker Compose** para orquestar m√∫ltiples servicios
2. **Explora** im√°genes oficiales (PostgreSQL, Redis, etc.)

---

## üìù Recursos adicionales

* **Documentaci√≥n oficial**: [docs.docker.com](https://docs.docker.com)
* **Docker Hub**: [hub.docker.com](https://hub.docker.com) - repositorio de im√°genes
* **Best practices**: Revisa las mejores pr√°cticas oficiales de Docker

---

> **Recuerda**: Docker no es solo para DevOps. Como Data Engineer, Docker te ayuda a crear pipelines m√°s confiables, reproducibles y f√°ciles de desplegar.
